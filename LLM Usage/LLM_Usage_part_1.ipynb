{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Spam Classification using an LLM\n",
    "\n",
    "In this notebook, we'll:\n",
    "\n",
    "1. Load a pre-trained instruction-following model from Hugging Face.\n",
    "2. Read the `spam.csv` dataset.\n",
    "3. Use prompt engineering to classify each email as **spam** or **ham** (not spam).\n",
    "\n",
    "We will ensure the model outputs **only** the class label for each email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.53.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (4.66.5)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "# 1. Install and import necessary libraries\n",
    "!pip install transformers pandas tqdm\n",
    "\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the LLM and create a text2text-generation pipeline\n",
    "\n",
    "We'll use `google/flan-t5-base`, which is a versatile Seq2Seq instruction-following model. The pipeline will generate a single-token output (`spam` or `ham`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05cabe4be7444e54aae3fea842b1f592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b66ea5b69e40119cb3d6903b5cb6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47cce87163df45b5a704b34857259d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0e58e49eee44f985335c3af0971546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7319f1e0754441be856a6fe4ea8b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2435bf33f844483181b9a5566bf2f1b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Pipeline zero-shot avec un modèle NLI\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    device=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the prompt template and classification function\n",
    "\n",
    "We craft a prompt that clearly instructs the model to output **only** the class label. We also constrain the generation to a short maximum length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = (\n",
    "    \"Perform a binary classification of the following email as 'spam' or 'ham'.\"\n",
    "    \" Respond with only one word: spam or ham.\\n\\n\"\n",
    "    \"Use your knowledge and common sense to classify the email.\"\n",
    "    \"Example 1:\\n\"\n",
    "    \"Email: Win cash prize now! Click here to claim your reward.\\n\"\n",
    "    \"Class: spam\\n\\n\"\n",
    "    \"Example 2:\\n\"\n",
    "    \"Email: Hi team, attached is the report for our meeting tomorrow.\\n\"\n",
    "    \"Class: ham\\n\\n\"\n",
    "    \"Email: {email_text}\\n\"\n",
    "    \"Class:\"\n",
    ")\n",
    "\n",
    "def classify_email(text: str) -> str:\n",
    "    res = classifier(\n",
    "        text, \n",
    "        candidate_labels=[\"spam\", \"ham\"],\n",
    "        multi_label=False\n",
    "    )\n",
    "    # res[\"labels\"] est trié par score décroissant\n",
    "    return res[\"labels\"][0].lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the dataset and apply classification\n",
    "\n",
    "We'll read `spam.csv`, classify each email, and add a new column with the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email #1: True=ham | Predicted=ham\n",
      "Email #2: True=ham | Predicted=spam\n",
      "Email #3: True=spam | Predicted=ham\n",
      "Email #4: True=ham | Predicted=ham\n",
      "Email #5: True=ham | Predicted=ham\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8adf7b4f48604f11a9c655b484be9c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifying emails:   0%|          | 0/5572 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "df.columns = ['text', 'target']\n",
    "\n",
    "# Quick sanity check on first 5 emails\n",
    "for idx, row in df.head(5).iterrows():\n",
    "    pred = classify_email(row['text'])\n",
    "    print(f\"Email #{idx+1}: True={row['target']} | Predicted={pred}\")\n",
    "\n",
    "# Classify\n",
    "tqdm.pandas(desc=\"Classifying emails\")\n",
    "df['predicted'] = df['text'].progress_apply(classify_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate and display sample results\n",
    "\n",
    "We'll compute the overall accuracy, F1-score, recall and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.28%\n",
      "Accuracy overall: 67.28%\n",
      "Classe  ham → Precision: 86.88%, Recall: 73.28%, F1-score: 79.51%\n",
      "Classe spam → Precision: 14.18%, Recall: 28.51%, F1-score: 18.94%\n",
      "\n",
      "Classification report détaillé :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham     0.8688    0.7328    0.7951      4825\n",
      "        spam     0.1418    0.2851    0.1894       747\n",
      "\n",
      "    accuracy                         0.6728      5572\n",
      "   macro avg     0.5053    0.5090    0.4922      5572\n",
      "weighted avg     0.7713    0.6728    0.7139      5572\n",
      "\n",
      "\n",
      "Matrice de confusion (lignes=vérité, colonnes=prédit) :\n",
      "[[3536 1289]\n",
      " [ 534  213]]\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy\n",
    "accuracy = (df['predicted'] == df['target']).mean()\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Show sample misclassifications\n",
    "df[df['predicted'] != df['target']].head(10)\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Supposons que vous avez déjà fait :\n",
    "df['target'] = df['target'].astype(str).str.strip().str.lower()\n",
    "df['predicted'] = df['predicted'].str.strip().str.lower()\n",
    "\n",
    "y_true = df['target']\n",
    "y_pred = df['predicted']\n",
    "\n",
    "# 1. Accuracy globale\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy overall: {acc:.2%}\")\n",
    "\n",
    "# 2. Précision, rappel et F1 par classe\n",
    "precisions = precision_score(y_true, y_pred, labels=['ham','spam'], average=None)\n",
    "rappels    = recall_score   (y_true, y_pred, labels=['ham','spam'], average=None)\n",
    "f1s        = f1_score       (y_true, y_pred, labels=['ham','spam'], average=None)\n",
    "\n",
    "for label, p, r, f in zip(['ham','spam'], precisions, rappels, f1s):\n",
    "    print(f\"Classe {label:>4} → Precision: {p:.2%}, Recall: {r:.2%}, F1-score: {f:.2%}\")\n",
    "\n",
    "# 3. Rapport de classification complet\n",
    "print(\"\\nClassification report détaillé :\")\n",
    "print(classification_report(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    labels=['ham','spam'], \n",
    "    target_names=['ham','spam'],\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# 4. Matrice de confusion\n",
    "cm = confusion_matrix(y_true, y_pred, labels=['ham','spam'])\n",
    "print(\"\\nMatrice de confusion (lignes=vérité, colonnes=prédit) :\")\n",
    "print(cm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
